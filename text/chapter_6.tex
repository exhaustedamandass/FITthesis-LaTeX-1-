% Evaluation %

\begin{chapterabstract}
This chapter presents the evaluation of MutatoR through case studies on real-world R packages that employ the \texttt{testthat} framework. We first detail the experimental setup—including VM configuration, R and \texttt{testthat} versions, and benchmarking tools—then apply MutatoR to a medium-sized package (\texttt{gtable}) and a large package (\texttt{dplyr}). For each case, we report total execution time, counts of AST-based and line-deletion mutants generated, numbers of killed and survived mutants, and the subset identified as equivalent. These metrics demonstrate how package complexity and test coverage influence MutatoR’s throughput and mutation scores.

\end{chapterabstract}

\section{System Specification}

All experiments were executed inside a Linux virtual machine provisioned via Lima on a MacBook Air M1 (Apple Silicon) host.  The VM was created with QEMU emulation in \texttt{aarch64} mode and configured as follows:

\begin{itemize}
  \item \textbf{CPUs:} 8 virtual cores  
  \item \textbf{Memory:} 16 GiB RAM  
  \item \textbf{Disk:} 200 GiB  
  \item \textbf{R version:} 4.3.x  
  \item \textbf{testthat version:} 3.1.x \cite{wickham2011testthat}  
  \item \textbf{MutatoR:} Latest GitHub commit at time of testing  
\end{itemize}

MutatoR itself was built and loaded via \texttt{devtools\allowbreak::\allowbreak load\_all()}, 
with all native code compiled under GCC 11.2.0. We used 
\texttt{microbenchmark\allowbreak::\allowbreak microbenchmark()} 
to measure per‐package execution times, and the \texttt{future} 
backend configured for multisession parallelism to stress‐test 
MutatoR’s parallel mutation and test‐execution pipeline.


\section{Case Studies}

To evaluate MutatoR across realistic workloads, we selected three R packages of varying size and complexity, all of which use the \texttt{testthat} framework for unit tests:

\begin{enumerate}
  \item \textbf{Small package:} \emph{pkgA} (≈ 5 R scripts, ~200 LOC, 15 tests)  
  \item \textbf{Medium package:} \emph{pkgB} (≈ 20 R scripts, ~1 200 LOC, 75 tests)  
  \item \textbf{Large package:} \emph{pkgC} (≈ 50 R scripts, ~5 000 LOC, 250 tests)  
\end{enumerate}

For each package, we ran \texttt{mutate\_package()} once and collected the following metrics:

\begin{itemize}
  \item \textbf{Total execution time:} Wall‐clock time from start to finish (including mutant generation, test execution, and equivalence detection).  
  \item \textbf{Generated mutants:} Total number of AST‐based and line‐deletion mutants produced.  
  \item \textbf{Killed mutants:} Number of mutants for which at least one test failed.  
  \item \textbf{Survived mutants:} Number of mutants that passed the entire test suite.  
  \item \textbf{Equivalent mutants (optional):} Among survivors, those marked “EQUIVALENT” by our LLM‐based analysis.
\end{itemize}

This setup allows us to assess how package size and test suite coverage affect MutatoR’s throughput, resource usage, and mutation score.  In the following subsections, we will present detailed results and discuss the trade‐offs observed across these case studies.

\subsection{Small Packages}

Small packages—typically under 300 lines of R code—provide a lightweight workload that tests MutatoR’s overhead and baseline performance. Their concise codebases and modest test suites allow us to observe how quickly mutants can be generated and assessed when the computational footprint is minimal.

\subsubsection{\texttt{oRaklE} Package}

The \texttt{oRaklE} package is a utility for random key–lock emulation in R. Its structure is:

\begin{itemize}
  \item \textbf{R source files:} 19 files in \texttt{R/}.  
  \item \textbf{Test files:} 1 \texttt{tests/testthat/} script with 1 test case.  
  \item \textbf{Dependencies:} Uses only base R and \texttt{testthat} (≥ 3.0.0) for testing.
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrrrr}
    \hline\hline
    Package & R files & LOC (R) & Test files & Test cases \\
    \hline
    \texttt{oRaklE} & 19 & 250 & 1 & 1 \\
    \hline
  \end{tabular}
  \caption{Characteristics of the \texttt{oRaklE} package used in the small‐size case study.}
  \label{tab:orakle-metrics}
\end{table}

\subsubsection{Results}

\subsection{Medium Packages}

To evaluate MutatoR on a typical medium‐sized codebase, we selected the \texttt{gtable} package—a layout engine for grid graphics that is widely used by \texttt{ggplot2} and other visualization tools \cite{rlib-gtable}.  \texttt{gtable} provides a representative workload with a moderate number of source files and a comprehensive \texttt{testthat} suite.

\subsubsection{\texttt{gtable} Package}

The \texttt{gtable} package (version 0.3.6) is structured as follows:

\begin{itemize}
  \item \textbf{R source files:} 13 files in the \texttt{R/} directory, totaling approximately 1\,200 lines of R code.
  \item \textbf{Test files:} 8 \texttt{tests/testthat/} scripts containing roughly 45 individual test cases.
  \item \textbf{Dependencies:} Imports \texttt{grid}, \texttt{rlang}, and others; Suggests \texttt{testthat} (≥ 3.0.0) for unit testing.
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrrrr}
    \hline\hline
    Package & R files & LOC (R) & Test files & Test cases \\
    \hline
    \texttt{gtable} & 13 & 1\,200 & 8 & 45 \\
    \hline
  \end{tabular}
  \caption{Characteristics of the \texttt{gtable} package used in the medium‐size case study.}
  \label{tab:gtable-metrics}
\end{table}

\subsubsection{Results}

The mutation run on the \texttt{gtable} package completed in approximately 5.72 s of sampled execution time (sampling interval = 0.02 s).  In total, MutatoR generated 243 mutants, of which 133 were killed by the existing \texttt{testthat} suite and 110 survived, yielding a raw mutation score of 54.73 \%.  A breakdown of key outcome metrics is shown in Table~\ref{tab:gtable-summary}.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    Metric & Value \\
    \hline
    Total execution time (sampled) & 5.72 s \\
    Total mutants & 243 \\
    Killed & 132 \\
    Survived & 111 \\
    Mutation score & 54.32 \% \\
    \hline
  \end{tabular}
  \caption{Mutation testing summary for \texttt{gtable}.}
  \label{tab:gtable-summary}
\end{table}

To understand where time was spent, we sampled profiling data during the mutation run.  Table~\ref{tab:gtable-profile} lists the top ten R functions by total execution time.  The majority of wall‐clock time is consumed by the parallel orchestration infrastructure (e.g.\ \texttt{mutate\_package}, \texttt{furrr\_map\_template}, \texttt{poll\_progress}) and by resolving futures for each mutant.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrr}
    \hline\hline
    Function & Total time (s) & Self time (s) \\
    \hline
    mutate\_package & 5.72 & 0.00 \\
    furrr\_map\_template & 4.84 & 0.00 \\
    poll\_progress & 3.76 & 0.00 \\
    any\_running & 3.62 & 0.02 \\
    resolved.list & 3.60 & 0.06 \\
    resolved.ClusterFuture & 3.54 & 0.24 \\
    connectionId & 1.96 & 0.08 \\
    capture.output & 1.56 & 0.06 \\
    serialize & 1.06 & 1.06 \\
    file.copy & 0.46 & 0.42 \\
    \hline
  \end{tabular}
  \caption{Top ten functions by total and self time in the \texttt{gtable} mutation run.}
  \label{tab:gtable-profile}
\end{table}

These results indicate that while mutant generation and test execution produce a significant number of AST‐based and string‐deletion mutants, the dominant cost is parallel task coordination and future resolution rather than the low‐level R operations themselves.

\subsection{Big Packages}

For a large‐scale evaluation, we chose the \texttt{stringr} package—a cornerstone of the tidyverse that provides a consistent set of wrappers for string operations in R and is known for its comprehensive, well‐documented codebase \cite{stringr}.

\subsubsection{\texttt{stringr} Package}

The \texttt{stringr} package (version 1.5.1) :contentReference[oaicite:0]{index=0} exhibits the following structure:

\begin{itemize}
  \item \textbf{R source files:} 35 scripts in \texttt{R/}, totaling roughly 4\,500 lines of code :contentReference[oaicite:1]{index=1}.
  \item \textbf{Test files:} Over 30 files in \texttt{tests/testthat/}, containing more than 60 individual test cases :contentReference[oaicite:2]{index=2}.
  \item \textbf{Dependencies:} Imports packages such as \texttt{stringi}, \texttt{rlang}, \texttt{glue}, \texttt{magrittr}, \texttt{vctrs}, \texttt{lifecycle}, and \texttt{cli}; suggests \texttt{testthat} (≥ 3.0.0), \texttt{knitr}, \texttt{rmarkdown}, and others for development and testing :contentReference[oaicite:3]{index=3}.
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrrrr}
    \hline\hline
    Package & R files & LOC (R) & Test files & Test cases \\
    \hline
    \texttt{stringr} & 35 & 4\,500 & 30+ & 60+ \\
    \hline
  \end{tabular}
  \caption{Characteristics of the \texttt{stringr} package used in the large‐size case study.}
  \label{tab:stringr-metrics}
\end{table}

\subsubsection{Results}

The mutation run on the \texttt{stringr} package completed in approximately 29.74~s of sampled execution time (sampling interval = 0.02~s).  In total, MutatoR generated 305 mutants, of which 212 were killed by the existing \texttt{testthat} suite and 93 survived, yielding a raw mutation score of 69.51\%.  A breakdown of key outcome metrics is shown in Table~\ref{tab:stringr-summary}.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    Metric & Value \\
    \hline
    Total execution time (sampled) & 29.74~s \\
    Total mutants                  & 305   \\
    Killed                         & 212   \\
    Survived                       & 93    \\
    Mutation score                 & 69.51\% \\
    \hline
  \end{tabular}
  \caption{Mutation testing summary for \texttt{stringr}.}
  \label{tab:stringr-summary}
\end{table}

To understand where time was spent, we sampled profiling data during the mutation run.  Table~\ref{tab:stringr-profile} lists the top ten R functions by total execution time.  The majority of wall‐clock time is consumed by the parallel orchestration infrastructure (e.g.\ \texttt{mutate\_package}, \texttt{furrr\_map\_template}, \texttt{poll\_progress}) and by resolving futures for each mutant.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrr}
    \hline\hline
    Function                   & Total time (s) & Self time (s) \\
    \hline
    \texttt{mutate\_package}           & 29.68          & 0.00           \\
    \texttt{furrr\_map\_template}      & 25.98          & 0.00           \\
    \texttt{poll\_progress}            & 24.52          & 0.18           \\
    \texttt{any\_running}              & 23.56          & 0.04           \\
    \texttt{future::resolved}          & 23.52          & 0.04           \\
    \texttt{resolved.list}             & 23.48          & 0.10           \\
    \texttt{resolved}                  & 23.30          & 0.58           \\
    \texttt{resolved.ClusterFuture}    & 22.72          & 1.56           \\
    \texttt{connectionId}              & 14.46          & 0.80           \\
    \texttt{capture.output}            & 10.82          & 0.82           \\
    \hline
  \end{tabular}
  \caption{Top ten functions by total and self time in the \texttt{stringr} mutation run.}
  \label{tab:stringr-profile}
\end{table}

\subsection{Comparison with Mutatr package}

%introduce the idea that we are comparing with Mutatr package as a main and only comparable product% 
 
\subsubsection{Results comparison}

% compare our results with results of Mutatr %

\section{Detection of Equivalent Mutants}

Building on the mutation runs presented above, we now evaluate the ability of large language models to identify equivalent mutants—those which survive testing yet do not change program behavior.  We focus on the \texttt{gtable} package, whose moderate size makes it both tractable for multiple API calls and representative of real‐world R libraries.

\subsection{Study Design}

To assess model performance and cost trade‐offs, we execute our equivalence detection pipeline on the same set of \texttt{gtable} mutants using three OpenAI models: \texttt{o4-mini}, \texttt{gpt-4.1}, and \texttt{o1}.  We then compare the quantitative results (counts of “EQUIVALENT”, “NOT EQUIVALENT”, and “DONT KNOW” judgments), analyze the quality of each model’s reasoning, and perform a targeted manual validation on a sample of cases from each model.  The study is structured as follows:

\subsubsection{Objectives}
\begin{itemize}
  \item Measure each model’s tendency to classify surviving mutants as equivalent vs.\ non‐equivalent vs.\ uncertain.
  \item Evaluate the clarity and correctness of the rationale provided in each model’s response.
  \item Compare cost (API calls) and latency implications of using lighter vs.\ heavier models.
  \item Manually verify a random subset of judgments to estimate precision and recall for equivalence detection.
\end{itemize}

\subsubsection{Package Under Test}
We reuse the \texttt{gtable} package (v0.3.6) and its 104 surviving mutants from Section~6.2.2.2.  It's 13 R scripts and 45 tests provide a controlled yet realistic workload, balancing the number of API requests against execution cost.
5
\subsubsection{Models and Configuration}
\begin{itemize}
  \item \textbf{o4-mini:} Low‐latency GPT‐4 variant; used for rapid, large‐scale screening.
  \item \textbf{gpt-4.1:} Full GPT‐4.1 model; expected to yield the most accurate equivalence judgments.
  \item \textbf{o1:} Lightweight GPT‐3.5–style model; serves as a cost‐effective baseline.
\end{itemize}
All calls use temperature = 0.1 and the same system prompt (“You are an expert in program analysis…”), ensuring comparability.

\subsubsection{Experimental Procedure}
\begin{enumerate}
  \item \textbf{Mutant Generation:} Use \texttt{mutate\_file} to produce the full set of surviving mutants for \texttt{gtable}.  
  \item \textbf{Prompt Construction:} For each model, invoke \texttt{identify\_equivalent\_mutants} to build and send prompts for all mutants in a single API session per file.  
  \item \textbf{Response Parsing:} Extract each mutant’s judgment (“EQUIVALENT”, “NOT EQUIVALENT”, or “DONT KNOW”) and record counts.  
  \item \textbf{Cost Logging:} Track total API tokens consumed and wall‐clock time per model.
\end{enumerate}

\subsubsection{Manual Validation}
From each model’s classified mutants, we randomly sample 10 mutants flagged as \texttt{EQUIVALENT} and 10 flagged as \texttt{NOT EQUIVALENT}.  Domain experts manually inspect the original and mutated code to confirm or refute the model’s judgment, enabling computation of precision, recall, and error patterns for each LLM.

This design enables a comprehensive comparison of model accuracy, reasoning quality, cost, and latency in the context of equivalent mutant detection on a real R

\subsubsection{Manual Evaluation of quality of detection of equivalent mutants}



\subsection{chatGPT 4.0-mini model results}

\subsubsection{Overall results}

\begin{table}[ht]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    \textbf{Metric}        & \textbf{Count / Score}       \\
    \hline
    Total mutants          & 243                          \\
    Killed                 & 131                          \\
    Survived               & 112                          \\
    Equivalent             & 0                            \\
    Not Equivalent         & 106                          \\
    Uncertain              & 6                            \\
    Mutation Score         & 53.91\%                      \\
    Adjusted Score         & 53.91\% (excluding equivalents) \\
    \hline
  \end{tabular}
  \caption{Summary of GPT-4.0-mini (\texttt{gpt-4.0-mini}) equivalence judgments on the \texttt{gtable} mutants}
  \label{tab:gpt40mini-overall}
\end{table}

GPT-4.0-mini processed all 243 mutants, correctly killing 131 and classifying 112 as survivors.  Of the survivors, it flagged 106 as “\texttt{NOT EQUIVALENT}” and expressed uncertainty on 6 cases, never declaring any mutant as equivalent.  This yields a raw mutation score of 53.91\%, which remains the same when adjusted (no equivalents to exclude).  Compared with GPT-4.1, GPT-4.0-mini adopts a more decisive stance—favoring “Not Equivalent” judgments with far fewer uncertain cases. 

\subsubsection{Manual evaluation of response}

Below we inspect three representative mutants for which the 4.0‐mini model produced judgments.

\paragraph{Manual evaluation of first mutant}
\begin{lstlisting}[
    float,
    caption={Original snippet in \texttt{trim.R} (trimmed)},
    label={lst:trim-layout},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
# Original calculation of left position
layout$l <- layout$l - w[1] + 1  # ← mutated to '+ w[1] + 1'
\end{lstlisting}

\paragraph{Mutant under inspection}
\begin{itemize}
  \item \textbf{Mutation:} changed  
    \texttt{layout\$l <- layout\$l - w[1] + 1}  
    to  
    \texttt{layout\$l <- layout\$l + w[1] + 1}
  \item \textbf{Answer received:} \texttt{NOT EQUIVALENT}
\end{itemize}

\paragraph{Answer received from model}
\begin{quote}
\texttt{NOT EQUIVALENT}
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    The model correctly pinpointed the single arithmetic operator change in the left‐position calculation.

  \item \textbf{Reasoning Quality:}  
    This mutation shifts every column’s left coordinate by an extra \(2\times w[1]\) units, altering the layout geometry. The model’s “\texttt{NOT EQUIVALENT}” verdict is therefore sound.

  \item \textbf{Validation and Test Coverage:}  
    No test currently asserts on the computed \texttt{layout\$l} under varied widths. A new test should verify that for a simple two‐column gtable, the left positions match the expected values when \texttt{trim()} is applied.
\end{enumerate}

\paragraph{Manual evaluation of second mutant}
\begin{lstlisting}[
    float,
    caption={Original input‐length check in \texttt{z\_arrange\_gtables} (trimmed)},
    label={lst:z-arrange-gtables},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
if (length(gtables) != length(z)) {  # ← this line was deleted
  cli::cli_abort("{.arg gtables} and {.arg z} must be the same length")
}
\end{lstlisting}

\paragraph{Mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{z.R\_z.R\_008.R}
  \item \textbf{Mutation:} deletion of the \texttt{cli::cli\_abort()} input validation
  \item \textbf{Answer received:} \texttt{DONT KNOW}
\end{itemize}

\paragraph{Answer received from model}
\begin{quote}
\texttt{DONT KNOW}
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    The model did not recognize that removing the validation call changes control‐flow: it failed to mark this as “\texttt{NOT EQUIVALENT}.”

  \item \textbf{Reasoning Quality:}  
    Deleting the \texttt{cli\_abort()} allows mismatched \texttt{gtables} and \texttt{z} lengths to proceed silently, leading to downstream indexing errors. This is clearly a semantic change.

  \item \textbf{Validation and Test Coverage:}  
    The test suite lacks any case with \(\texttt{length(gtables)} \neq \texttt{length(z)}\). Adding a test that passes vectors of different lengths and expects an error would catch this mutant.
\end{enumerate}

\paragraph{Manual evaluation of third mutant}
\begin{lstlisting}[
    float,
    caption={Original input‐type check in \texttt{import\_standalone\_obj\_type.R} (trimmed)},
    label={lst:import-standalone-obj-type},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\ttfamily\small,
    breaklines=true,
    captionpos=b
]
if (!n_dim) {
  if (!is_list(x) && length(x) == 1) {       # ← mutated '==' → '!='
    if (is_na(x)) {
      return(switch(
        typeof(x),
        ...
      ))
    }
  }
}
\end{lstlisting}

\paragraph{Mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{import-standalone-obj-type.R\_import-standalone-obj-type.R\_002.R}
  \item \textbf{Mutation:} changed the length check from \texttt{length(x) == 1} to \texttt{length(x) != 1}
  \item \textbf{Answer received:} \texttt{NOT EQUIVALENT}
\end{itemize}

\paragraph{Answer received from model}
\begin{quote}
\texttt{NOT EQUIVALENT}
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    The model correctly detected the flipped equality operator in the `length(x)` condition, which reverses the branch trigger.

  \item \textbf{Reasoning Quality:}  
    Now the function handles all inputs except single, non‐list values, fundamentally reversing control‐flow. The “\texttt{NOT EQUIVALENT}” verdict is therefore correct.

  \item \textbf{Validation and Test Coverage:}  
    No tests cover the case of a single NA scalar versus other lengths under `!n\_dim`. Adding tests for both scenarios would expose this mutant.
\end{enumerate}


\subsection{chaGPT 4.1 model results}

\subsubsection{Overall results}

\begin{table}[ht]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    \textbf{Metric} & \textbf{Count / Score} \\
    \hline
    Total mutants & 243 \\
    Killed & 132 \\
    Survived & 111 \\
    Equivalent & 0 \\
    Not Equivalent & 56 \\
    Uncertain & 55 \\
    Mutation Score & 54.32\% \\
    Adjusted Score & 54.32\% (excluding equivalents) \\
    \hline
  \end{tabular}
  \caption{Summary of GPT-4.1 (\texttt{gpt-4.1}) equivalence judgments on the \texttt{gtable} mutants}
  \label{tab:gpt41-overall}
\end{table}

GPT-4.1 processed all 243 mutants, correctly killing 132 and classifying 111 as survivors.  Of the survivors, it flagged 56 as “\texttt{NOT EQUIVALENT}” and expressed uncertainty on 55 cases, never declaring any mutant as equivalent.  This yields a raw mutation score of 54.32\%, which remains the same when adjusted (no equivalents to exclude).  Compared with lighter models, GPT-4.1 shows a cautious stance (near 50/50 split between non-equivalent and uncertain), suggesting high precision at the expense of recall.

\subsubsection{Manual evaluation of response}

Below we inspect one representative surviving mutant (ID \texttt{filter.R\_filter.R\_003.R}), for which GPT-4.1 answered \texttt{NOT EQUIVALENT}.  Some of the original source code was trimmed for brevity.

\begin{lstlisting}[
    float,
    caption={Original \texttt{gtable\_filter} (trimmed)},
    label={lst:gtable-filter},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
gtable_filter <- function(x, pattern, fixed = FALSE, trim = TRUE, invert = FALSE) {
  matches <- grepl(pattern, .subset2(x$layout, "name"), fixed = fixed)
  if (invert) matches <- !matches
  x$layout <- x$layout[matches, , drop = FALSE]
  x$grobs   <- x$grobs[matches]

  if (trim) x <- gtable_trim(x)   # ← line 42, deleted by mutant

  x
}
\end{lstlisting}


\paragraph{Mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{filter.R\_filter.R\_003.R}
  \item \textbf{Mutation:} deletion of line 42 (\texttt{if (trim) x <- gtable\_trim(x)})
\end{itemize}

\paragraph{Answer received from GPT-4.1}
\begin{quote}
**Assessment of Mutant:**

-   \item \textbf{Mutant ID:} \texttt{filter.R\_filter.R\_003.R}  
- **Mutation:** deleted line 42

**Line 42 in the original code:**
\begin{lstlisting}[
    float,
    caption={Conditional call to \texttt{gtable\_trim} when \texttt{trim} is TRUE},
    label={lst:gtable-filter-trim-line},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
if (trim) x <- gtable_trim(x)
\end{lstlisting}


**Effect of the mutation:**  
The mutant removes the call to \texttt{gtable\_trim(x)} when \texttt{trim} is \texttt{TRUE}. This means the mutant will never trim empty rows/columns, regardless of the value of the \texttt{trim} argument.

**Behavioral difference:**  
For any input where filtering leaves empty rows or columns and `trim = TRUE`, the original will remove them, but the mutant will not.

**Conclusion:**  
**NOT EQUIVALENT**
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    GPT-4.1 correctly located the mutated line inside \texttt{gtable\_filter} and quoted it verbatim, demonstrating precise pinpointing of the defect.
  
     \item \textbf{Reasoning Quality:}  
    The model’s analysis clearly explains that removing the \texttt{gtable\_trim} call alters behavior whenever empty rows/columns would otherwise be removed. This logical chain is sound and complete.


  \item \textbf{Validation and Test Coverage:}  
    Although GPT-4.1 correctly concludes “\texttt{NOT EQUIVALENT},” the existing test suite for \texttt{gtable} does not include a case where all grobs in a row/column are filtered out with `trim = TRUE`.  Hence, this mutant remains “survived” under current tests, making it a valid mutant for further testing or inclusion in a new test case.
\end{enumerate}

Overall, the manual check confirms GPT-4.1’s pinpointing and reasoning are both correct and useful, while also highlighting a gap in test coverage that the tool has exposed.

\paragraph{Second mutant under inspection}
\begin{itemize}
    \item \textbf{Mutant ID:} 001       
    \item \textbf{Mutation:} logical operator changed from \texttt{\&\&} to \texttt{||} in \texttt{check\_bool}
    \item \textbf{Original code (lines 55–74, trimmed):}
\begin{lstlisting}[language=R]

  check_bool <- function(x, allow_na = FALSE, allow_null = FALSE) {
    if (!missing(x) && .standalone_types_check_dot_call(
          ffi_standalone_is_bool_1.0.7, x, allow_na, allow_null)) {
      return(invisible(NULL))
    }
    # … rest of function …
  }
  \end{lstlisting}
  \item \textbf{Mutated code:}
  \begin{lstlisting}[language=R]
    if (!missing(x) || .standalone_types_check_dot_call(
          ffi_standalone_is_bool_1.0.7, x, allow_na, allow_null)) {
      return(invisible(NULL))
    }
  \end{lstlisting}
\end{itemize}

\paragraph{Answer received from GPT-4.1}
\begin{quote}
**Assessment of Mutant:**

- **Mutant ID:** 001  
- \begin{itemize}
  \item \textbf{Mutation:} changed \texttt{\&\&} to \texttt{||} in the \texttt{if} condition
\end{itemize}


**Effect of the mutation:**  
The mutant’s condition will return early if \emph{either} `x` is provided (regardless of the C call’s result) \emph{or} the C call succeeds (even if `x` is missing).  

**Behavioral difference:**  
- Original: early return only when `x` is present \emph{and} passes the C-level boolean check.  
- Mutant: early return also when `x` is provided but fails the C check, or when `x` is missing but the C check (on `NULL`) passes (if that ever occurs).

**Conclusion:**  
**NOT EQUIVALENT**
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    GPT-4.1 correctly spotted the operator swap in the `if` condition and cited the precise code change.
  
  \item \textbf{Reasoning Quality:}  
    The model’s explanation clearly contrasts the two logical paths and illustrates how the mutant condition diverges from the original in edge cases (missing `x` or failed C-call).
  
  \item \textbf{Validation and Test Coverage:}  
    In practice, this mutant will alter behavior for inputs where `x` is provided but not a boolean, causing an early return instead of proceeding to error handling, and possibly for missing `x` if the C check handles `NULL`. Since the existing test suite does not cover these specific edge cases, the mutant remains “survived,” confirming its validity and highlighting another gap in coverage.
\end{enumerate}

\paragraph{Third mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} new-data-frame.R\_new-data-frame.R\_002.R  
  \item \textbf{Mutation:} changed logical operator from \texttt{!=} to \texttt{==} in \texttt{validate\_data\_frame}
  \item \textbf{Original code (lines 14–21, trimmed):}
  \begin{lstlisting}[language=R]
  validate_data_frame <- function(x) {
    if (length(unique(lengths(x))) != 1) {
      cli::cli_abort('All elements in a data.frame must be of equal length')
    }
    if (is.null(names(x))) {
      cli::cli_abort('columns must be named')
    }
  }
  \end{lstlisting}
  \item \textbf{Mutated code:}
  \begin{lstlisting}[language=R]
    if (length(unique(lengths(x))) == 1) {
  \end{lstlisting}
\end{itemize}

\paragraph{Answer received from GPT-4.1}
\begin{quote}
DONT KNOW  
\textbf{Reasoning:}  
The change flips the error condition—now aborting when lengths are all equal rather than when they differ. Without knowledge of downstream usage, intended preconditions, or exhaustive test cases, it’s impossible to determine if this mutation preserves observable behavior under all valid inputs.
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    GPT-4.1 accurately located and described the operator swap (`!=` → `==`) in the length‐check conditional.
  
  \item \textbf{Reasoning Quality:}  
    The model correctly recognizes that deciding equivalence would require complete specification of valid inputs and outputs; expressing “DONT KNOW” here is prudent.
  
  \item \textbf{Validation and Test Coverage:}  
    No existing tests verify both cases where columns are uniform and where they differ in tandem with name checks. Lacking a formal spec or additional test cases, one cannot conclusively judge equivalence. Hence, the LLM’s uncertainty is justified.
\end{enumerate}

\subsection{chatGPT o1 model results}

\subsubsection{Overall results}

\begin{table}[ht]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    \textbf{Metric}        & \textbf{Count / Score}           \\
    \hline
    Total mutants          & 243                              \\
    Killed                 & 140                              \\
    Survived               & 103                              \\
    Equivalent             & 1                                \\
    Not Equivalent         & 11                               \\
    Uncertain              & 91                               \\
    Mutation Score         & 57.61\%                          \\
    Adjusted Score         & 57.85\% (excluding equivalents)  \\
    \hline
  \end{tabular}
  \caption{Summary of O1 (\texttt{o1}) equivalence judgments on the \texttt{gtable} mutants}
  \label{tab:o1-overall}
\end{table}

O1 processed all 243 mutants, correctly killing 140 and classifying 103 as survivors.  Of the survivors, it flagged 11 as “\texttt{NOT EQUIVALENT},” expressed uncertainty on 91 cases, and declared 1 mutant as equivalent.  This yields a raw mutation score of 57.61\%, which adjusts slightly upward to 57.85\% when the single equivalent mutant is excluded.  Compared with GPT-4.0-mini, O1 takes a notably more cautious approach, with the bulk of survivors marked “Uncertain” rather than definitively non-equivalent. 

\subsubsection{Manual evaluation of response}

Below we inspect the first surviving mutant (ID \texttt{add-space.R\_add-space.R\_014.R}), for which the model answered \texttt{EQUIVALENT}.  Some of the original source code was trimmed for brevity.

\begin{lstlisting}[
    float,
    caption={Original \texttt{gtable\_add\_col\_space} (trimmed)},
    label={lst:gtable-add-col-space},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
gtable_add_col_space <- function(x, width) {
  check_gtable(x)
  n <- length(x$widths) - 1
  if (n == 0) return(x)

  if (!(length(width) == 1 || length(width) == n)) {
    cli::cli_abort("{.arg width} must be of length 1 or ncol - 1")
  }
  width <- rep(width, length.out = n)

  for (i in rev(seq_len(n))) {
    x <- gtable_add_cols(x, width[i], pos = i)
  }

  x  # ← line 38
}
\end{lstlisting}

\paragraph{Mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{add-space.R\_add-space.R\_014.R}
  \item \textbf{Mutation:} deletion of line 38 (the final `x` return)
\end{itemize}

\paragraph{Answer received from model}
\begin{quote}
\texttt{EQUIVALENT}
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    The model treated line 38 as inconsequential whitespace, missing that it contains the explicit return of the modified \texttt{x} object, so its deletion suppresses the final return.

  \item \textbf{Reasoning Quality:}  
    In R, omitting an explicit final expression changes which value is returned (and under what visibility), so removing that line alters the function’s semantics and could break downstream method dispatch or side‐effects.

  \item \textbf{Validation and Test Coverage:}  
    No existing test verifies that \texttt{gtable\_add\_col\_space} actually returns a proper \texttt{gtable} after adding columns; adding a test to assert class and content of the returned object when multiple spaces are inserted would catch this change.
\end{enumerate}

\paragraph{Second mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{utils.R\_utils.R\_001.R}
  \item \textbf{Mutation:} logical operator changed from \texttt{\&\&} to \texttt{||} in the `allow\_null` check
  \item \textbf{Original code (trimmed):}
    \begin{lstlisting}[language=R]
if (!missing(x)) {
  if (is.gtable(x)) {
    return(invisible(NULL))
  }
  if (allow_null && is_null(x)) {
    return(invisible(NULL))
  }
}
    \end{lstlisting}
  \item \textbf{Mutated code:}
    \begin{lstlisting}[language=R]
if (!missing(x)) {
  if (is.gtable(x)) {
    return(invisible(NULL))
  }
  if (allow_null || is_null(x)) {
    return(invisible(NULL))
  }
}
    \end{lstlisting}
\end{itemize}

\paragraph{Answer received from model}
\begin{quote}
\texttt{NOT EQUIVALENT}
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    The model correctly spotted that switching from \texttt{\&\&} to \texttt{||} means the early return now triggers when either `allow\_null` is \texttt{TRUE} or `x` is \texttt{NULL}, instead of requiring both.

  \item \textbf{Reasoning Quality:}  
    This inversion changes behavior for cases where `allow\_null = TRUE` but `x` is non‐null (mutant returns early incorrectly), so the logical impact is well understood.

  \item \textbf{Validation and Test Coverage:}  
    The test suite lacks a case combining `allow\_null = TRUE` with a non‐null `x`; adding such a test would expose this mutant’s changed behavior.
\end{enumerate}

\paragraph{Third mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{add-grob.R\_add-grob.R\_001.R}
  \item \textbf{Mutation:} logical operator changed from \texttt{||} to \texttt{\&\&} in the input validation
  \item \textbf{Original code (trimmed):}
    \begin{lstlisting}[language=R]
if (is.grob(grobs)) {
  grobs <- list(grobs)
}
if (!is_list(grobs) || any(!vapply(grobs, is.grob, logical(1)))) {
  stop_input_type(grobs, "a single grob or a list of grobs")
}
n_grobs <- length(grobs)
    \end{lstlisting}
  \item \textbf{Mutated code:}
    \begin{lstlisting}[language=R]
if (!is_list(grobs) && any(!vapply(grobs, is.grob, logical(1)))) {
  stop_input_type(grobs, "a single grob or a list of grobs")
}
    \end{lstlisting}
\end{itemize}

\paragraph{Answer received from model}
\begin{quote}
\texttt{NOT EQUIVALENT}
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    The model correctly identified that changing \texttt{||} to \texttt{\&\&} means the error is now thrown only if \emph{both} conditions hold, rather than if either does.

  \item \textbf{Reasoning Quality:}  
    This swap allows many invalid inputs (non‐list or mixed‐type lists) to bypass the check, so the model’s judgment of behavioral change is sound.

  \item \textbf{Validation and Test Coverage:}  
    No tests currently feed a non‐list or a mixed‐type list of grobs; adding such cases would catch this semantic alteration and kill the mutant.
\end{enumerate}
