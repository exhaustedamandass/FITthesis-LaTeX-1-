% Evaluation %

\begin{chapterabstract}
This chapter presents the evaluation of MutatoR through case studies on real-world R packages that employ the \texttt{testthat} framework. We first detail the experimental setup—including VM configuration, R and \texttt{testthat} versions, and benchmarking tools—then apply MutatoR to a medium-sized package (\texttt{gtable}) and a large package (\texttt{dplyr}). For each case, we report total execution time, counts of AST-based and line-deletion mutants generated, numbers of killed and survived mutants, and the subset identified as equivalent. These metrics demonstrate how package complexity and test coverage influence MutatoR’s throughput and mutation scores.

\end{chapterabstract}

\section{System Specification}

All experiments were executed inside a Linux virtual machine provisioned via Lima on a MacBook Air M1 (Apple Silicon) host.  The VM was created with QEMU emulation in \texttt{aarch64} mode and configured as follows:

\begin{itemize}
  \item \textbf{CPUs:} 8 virtual cores  
  \item \textbf{Memory:} 16 GiB RAM  
  \item \textbf{Disk:} 200 GiB  
  \item \textbf{R version:} 4.3.x  
  \item \textbf{testthat version:} 3.1.x \cite{wickham2011testthat}  
  \item \textbf{MutatoR:} Latest GitHub commit at time of testing  
\end{itemize}

MutatoR itself was built and loaded via \texttt{devtools\allowbreak::\allowbreak load\_all()}, 
with all native code compiled under GCC 11.2.0. We used 
\texttt{microbenchmark\allowbreak::\allowbreak microbenchmark()} 
to measure per‐package execution times, and the \texttt{future} 
backend configured for multisession parallelism to stress‐test 
MutatoR’s parallel mutation and test‐execution pipeline.


\section{Case Studies}

To evaluate MutatoR across realistic workloads, we selected three R packages of varying size and complexity, all of which use the \texttt{testthat} framework for unit tests:

\begin{enumerate}
  \item \textbf{Small package:} \emph{pkgA} (≈ 5 R scripts, ~200 LOC, 15 tests)  
  \item \textbf{Medium package:} \emph{pkgB} (≈ 20 R scripts, ~1 200 LOC, 75 tests)  
  \item \textbf{Large package:} \emph{pkgC} (≈ 50 R scripts, ~5 000 LOC, 250 tests)  
\end{enumerate}

For each package, we ran \texttt{mutate\_package()} once and collected the following metrics:

\begin{itemize}
  \item \textbf{Total execution time:} Wall‐clock time from start to finish (including mutant generation, test execution, and equivalence detection).  
  \item \textbf{Generated mutants:} Total number of AST‐based and line‐deletion mutants produced.  
  \item \textbf{Killed mutants:} Number of mutants for which at least one test failed.  
  \item \textbf{Survived mutants:} Number of mutants that passed the entire test suite.  
  \item \textbf{Equivalent mutants (optional):} Among survivors, those marked “EQUIVALENT” by our LLM‐based analysis.
\end{itemize}

This setup allows us to assess how package size and test suite coverage affect MutatoR’s throughput, resource usage, and mutation score.  In the following subsections, we will present detailed results and discuss the trade‐offs observed across these case studies.

\subsection{Small Packages}

Small packages—typically under 300 lines of R code—provide a lightweight workload that tests MutatoR’s overhead and baseline performance. Their concise codebases and modest test suites allow us to observe how quickly mutants can be generated and assessed when the computational footprint is minimal.

\subsubsection{\texttt{oRaklE} Package}

The \texttt{oRaklE} package is a utility for random key–lock emulation in R. Its structure is:

\begin{itemize}
  \item \textbf{R source files:} 19 files in \texttt{R/}.  
  \item \textbf{Test files:} 1 \texttt{tests/testthat/} script with 1 test case.  
  \item \textbf{Dependencies:} Uses only base R and \texttt{testthat} (≥ 3.0.0) for testing.
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrrrr}
    \hline\hline
    Package & R files & LOC (R) & Test files & Test cases \\
    \hline
    \texttt{oRaklE} & 19 & 250 & 1 & 1 \\
    \hline
  \end{tabular}
  \caption{Characteristics of the \texttt{oRaklE} package used in the small‐size case study.}
  \label{tab:orakle-metrics}
\end{table}

\subsubsection{Results}

\subsection{Medium Packages}

To evaluate MutatoR on a typical medium‐sized codebase, we selected the \texttt{gtable} package—a layout engine for grid graphics that is widely used by \texttt{ggplot2} and other visualization tools \cite{rlib-gtable}.  \texttt{gtable} provides a representative workload with a moderate number of source files and a comprehensive \texttt{testthat} suite.

\subsubsection{\texttt{gtable} Package}

The \texttt{gtable} package (version 0.3.6) is structured as follows:

\begin{itemize}
  \item \textbf{R source files:} 13 files in the \texttt{R/} directory, totaling approximately 1\,200 lines of R code.
  \item \textbf{Test files:} 8 \texttt{tests/testthat/} scripts containing roughly 45 individual test cases.
  \item \textbf{Dependencies:} Imports \texttt{grid}, \texttt{rlang}, and others; Suggests \texttt{testthat} (≥ 3.0.0) for unit testing.
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrrrr}
    \hline\hline
    Package & R files & LOC (R) & Test files & Test cases \\
    \hline
    \texttt{gtable} & 13 & 1\,200 & 8 & 45 \\
    \hline
  \end{tabular}
  \caption{Characteristics of the \texttt{gtable} package used in the medium‐size case study.}
  \label{tab:gtable-metrics}
\end{table}

\subsubsection{Results}

The mutation run on the \texttt{gtable} package completed in approximately 5.72 s of sampled execution time (sampling interval = 0.02 s).  In total, MutatoR generated 243 mutants, of which 133 were killed by the existing \texttt{testthat} suite and 110 survived, yielding a raw mutation score of 54.73 \%.  A breakdown of key outcome metrics is shown in Table~\ref{tab:gtable-summary}.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    Metric & Value \\
    \hline
    Total execution time (sampled) & 5.72 s \\
    Total mutants & 243 \\
    Killed & 132 \\
    Survived & 111 \\
    Mutation score & 54.32 \% \\
    \hline
  \end{tabular}
  \caption{Mutation testing summary for \texttt{gtable}.}
  \label{tab:gtable-summary}
\end{table}

To understand where time was spent, we sampled profiling data during the mutation run.  Table~\ref{tab:gtable-profile} lists the top ten R functions by total execution time.  The majority of wall‐clock time is consumed by the parallel orchestration infrastructure (e.g.\ \texttt{mutate\_package}, \texttt{furrr\_map\_template}, \texttt{poll\_progress}) and by resolving futures for each mutant.

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrr}
    \hline\hline
    Function & Total time (s) & Self time (s) \\
    \hline
    mutate\_package & 5.72 & 0.00 \\
    furrr\_map\_template & 4.84 & 0.00 \\
    poll\_progress & 3.76 & 0.00 \\
    any\_running & 3.62 & 0.02 \\
    resolved.list & 3.60 & 0.06 \\
    resolved.ClusterFuture & 3.54 & 0.24 \\
    connectionId & 1.96 & 0.08 \\
    capture.output & 1.56 & 0.06 \\
    serialize & 1.06 & 1.06 \\
    file.copy & 0.46 & 0.42 \\
    \hline
  \end{tabular}
  \caption{Top ten functions by total and self time in the \texttt{gtable} mutation run.}
  \label{tab:gtable-profile}
\end{table}

These results indicate that while mutant generation and test execution produce a significant number of AST‐based and string‐deletion mutants, the dominant cost is parallel task coordination and future resolution rather than the low‐level R operations themselves.

\subsection{Big Packages}

For a large‐scale evaluation, we chose the \texttt{dplyr} package—a cornerstone of the tidyverse that provides a grammar of data manipulation and is known for its extensive, well‐tested codebase \cite{dplyr}.

\subsubsection{\texttt{dplyr} Package}

The \texttt{dplyr} package (version 1.1.1) exhibits the following structure:

\begin{itemize}
  \item \textbf{R source files:}  sixty–plus scripts in \texttt{R/}, totaling roughly 7\,000 lines of code.
  \item \textbf{Test files:} Over 100 \texttt{tests/testthat/} files with more than 300 individual test cases, reflecting its reputation for thorough testing.
  \item \textbf{Dependencies:} Imports numerous packages (e.g., \texttt{tibble}, \texttt{rlang}); Suggests \texttt{testthat} (≥ 3.0.0) and others for development and testing.
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lrrrr}
    \hline\hline
    Package & R files & LOC (R) & Test files & Test cases \\
    \hline
    \texttt{dplyr} & 60+ & 7\,000 & 100+ & 300+ \\
  \end{tabular}
  \caption{Characteristics of the \texttt{dplyr} package used in the large‐size case study.}
  \label{tab:dplyr-metrics}
\end{table}

\subsubsection{Results}

% Results for \texttt{dplyr} will be presented here, including execution time, number of mutants generated, killed, survived, and equivalent mutants.
\subsection{Comparison with Mutatr package}

%introduce the idea that we are comparing with Mutatr package as a main and only comparable product% 
 
\subsubsection{Results comparison}

% compare our results with results of Mutatr %

\section{Detection of Equivalent Mutants}

Building on the mutation runs presented above, we now evaluate the ability of large language models to identify equivalent mutants—those which survive testing yet do not change program behavior.  We focus on the \texttt{gtable} package, whose moderate size makes it both tractable for multiple API calls and representative of real‐world R libraries.

\subsection{Study Design}

To assess model performance and cost trade‐offs, we execute our equivalence detection pipeline on the same set of \texttt{gtable} mutants using three OpenAI models: \texttt{o4-mini}, \texttt{gpt-4.1}, and \texttt{o1}.  We then compare the quantitative results (counts of “EQUIVALENT”, “NOT EQUIVALENT”, and “DONT KNOW” judgments), analyze the quality of each model’s reasoning, and perform a targeted manual validation on a sample of cases from each model.  The study is structured as follows:

\subsubsection{Objectives}
\begin{itemize}
  \item Measure each model’s tendency to classify surviving mutants as equivalent vs.\ non‐equivalent vs.\ uncertain.
  \item Evaluate the clarity and correctness of the rationale provided in each model’s response.
  \item Compare cost (API calls) and latency implications of using lighter vs.\ heavier models.
  \item Manually verify a random subset of judgments to estimate precision and recall for equivalence detection.
\end{itemize}

\subsubsection{Package Under Test}
We reuse the \texttt{gtable} package (v0.3.6) and its 104 surviving mutants from Section~6.2.2.2.  It's 13 R scripts and 45 tests provide a controlled yet realistic workload, balancing the number of API requests against execution cost.
5
\subsubsection{Models and Configuration}
\begin{itemize}
  \item \textbf{o4-mini:} Low‐latency GPT‐4 variant; used for rapid, large‐scale screening.
  \item \textbf{gpt-4.1:} Full GPT‐4.1 model; expected to yield the most accurate equivalence judgments.
  \item \textbf{o1:} Lightweight GPT‐3.5–style model; serves as a cost‐effective baseline.
\end{itemize}
All calls use temperature = 0.1 and the same system prompt (“You are an expert in program analysis…”), ensuring comparability.

\subsubsection{Experimental Procedure}
\begin{enumerate}
  \item \textbf{Mutant Generation:} Use \texttt{mutate\_file} to produce the full set of surviving mutants for \texttt{gtable}.  
  \item \textbf{Prompt Construction:} For each model, invoke \texttt{identify\_equivalent\_mutants} to build and send prompts for all mutants in a single API session per file.  
  \item \textbf{Response Parsing:} Extract each mutant’s judgment (“EQUIVALENT”, “NOT EQUIVALENT”, or “DONT KNOW”) and record counts.  
  \item \textbf{Cost Logging:} Track total API tokens consumed and wall‐clock time per model.
\end{enumerate}

\subsubsection{Manual Validation}
From each model’s classified mutants, we randomly sample 10 mutants flagged as \texttt{EQUIVALENT} and 10 flagged as \texttt{NOT EQUIVALENT}.  Domain experts manually inspect the original and mutated code to confirm or refute the model’s judgment, enabling computation of precision, recall, and error patterns for each LLM.

This design enables a comprehensive comparison of model accuracy, reasoning quality, cost, and latency in the context of equivalent mutant detection on a real R

\subsubsection{Manual Evaluation of quality of detection of equivalent mutants}



\subsection{chatGPT 4.0-mini model results}

\subsection{chaGPT 4.1 model results}

\subsubsection{Overall results}

\begin{table}[ht]
  \centering
  \begin{tabular}{lr}
    \hline\hline
    \textbf{Metric} & \textbf{Count / Score} \\
    \hline
    Total mutants & 243 \\
    Killed & 132 \\
    Survived & 111 \\
    Equivalent & 0 \\
    Not Equivalent & 56 \\
    Uncertain & 55 \\
    Mutation Score & 54.32\% \\
    Adjusted Score & 54.32\% (excluding equivalents) \\
    \hline
  \end{tabular}
  \caption{Summary of GPT-4.1 (\texttt{gpt-4.1}) equivalence judgments on the \texttt{gtable} mutants}
  \label{tab:gpt41-overall}
\end{table}

GPT-4.1 processed all 243 mutants, correctly killing 132 and classifying 111 as survivors.  Of the survivors, it flagged 56 as “\texttt{NOT EQUIVALENT}” and expressed uncertainty on 55 cases, never declaring any mutant as equivalent.  This yields a raw mutation score of 54.32\%, which remains the same when adjusted (no equivalents to exclude).  Compared with lighter models, GPT-4.1 shows a cautious stance (near 50/50 split between non-equivalent and uncertain), suggesting high precision at the expense of recall.

\subsubsection{Manual evaluation of response}

Below we inspect one representative surviving mutant (ID \texttt{filter.R\_filter.R\_003.R}), for which GPT-4.1 answered \texttt{NOT EQUIVALENT}.  Some of the original source code was trimmed for brevity.

\begin{lstlisting}[
    float,
    caption={Original \texttt{gtable\_filter} (trimmed)},
    label={lst:gtable-filter},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
gtable_filter <- function(x, pattern, fixed = FALSE, trim = TRUE, invert = FALSE) {
  matches <- grepl(pattern, .subset2(x$layout, "name"), fixed = fixed)
  if (invert) matches <- !matches
  x$layout <- x$layout[matches, , drop = FALSE]
  x$grobs   <- x$grobs[matches]

  if (trim) x <- gtable_trim(x)   # ← line 42, deleted by mutant

  x
}
\end{lstlisting}


\paragraph{Mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} \texttt{filter.R\_filter.R\_003.R}
  \item \textbf{Mutation:} deletion of line 42 (\texttt{if (trim) x <- gtable\_trim(x)})
\end{itemize}

\paragraph{Answer received from GPT-4.1}
\begin{quote}
**Assessment of Mutant:**

-   \item \textbf{Mutant ID:} \texttt{filter.R\_filter.R\_003.R}  
- **Mutation:** deleted line 42

**Line 42 in the original code:**
\begin{lstlisting}[
    float,
    caption={Conditional call to \texttt{gtable\_trim} when \texttt{trim} is TRUE},
    label={lst:gtable-filter-trim-line},
    language=R,
    abovecaptionskip=-\medskipamount,
    belowcaptionskip=\medskipamount,
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b
]
if (trim) x <- gtable_trim(x)
\end{lstlisting}


**Effect of the mutation:**  
The mutant removes the call to \texttt{gtable\_trim(x)} when \texttt{trim} is \texttt{TRUE}. This means the mutant will never trim empty rows/columns, regardless of the value of the \texttt{trim} argument.

**Behavioral difference:**  
For any input where filtering leaves empty rows or columns and `trim = TRUE`, the original will remove them, but the mutant will not.

**Conclusion:**  
**NOT EQUIVALENT**
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    GPT-4.1 correctly located the mutated line inside \texttt{gtable\_filter} and quoted it verbatim, demonstrating precise pinpointing of the defect.
  
     \item \textbf{Reasoning Quality:}  
    The model’s analysis clearly explains that removing the \texttt{gtable\_trim} call alters behavior whenever empty rows/columns would otherwise be removed. This logical chain is sound and complete.


  \item \textbf{Validation and Test Coverage:}  
    Although GPT-4.1 correctly concludes “\texttt{NOT EQUIVALENT},” the existing test suite for \texttt{gtable} does not include a case where all grobs in a row/column are filtered out with `trim = TRUE`.  Hence, this mutant remains “survived” under current tests, making it a valid mutant for further testing or inclusion in a new test case.
\end{enumerate}

Overall, the manual check confirms GPT-4.1’s pinpointing and reasoning are both correct and useful, while also highlighting a gap in test coverage that the tool has exposed.

\paragraph{Second mutant under inspection}
\begin{itemize}
    \item \textbf{Mutant ID:} 001       
    \item \textbf{Mutation:} logical operator changed from \texttt{\&\&} to \texttt{||} in \texttt{check\_bool}
    \item \textbf{Original code (lines 55–74, trimmed):}
\begin{lstlisting}[language=R]

  check_bool <- function(x, allow_na = FALSE, allow_null = FALSE) {
    if (!missing(x) && .standalone_types_check_dot_call(
          ffi_standalone_is_bool_1.0.7, x, allow_na, allow_null)) {
      return(invisible(NULL))
    }
    # … rest of function …
  }
  \end{lstlisting}
  \item \textbf{Mutated code:}
  \begin{lstlisting}[language=R]
    if (!missing(x) || .standalone_types_check_dot_call(
          ffi_standalone_is_bool_1.0.7, x, allow_na, allow_null)) {
      return(invisible(NULL))
    }
  \end{lstlisting}
\end{itemize}

\paragraph{Answer received from GPT-4.1}
\begin{quote}
**Assessment of Mutant:**

- **Mutant ID:** 001  
- \begin{itemize}
  \item \textbf{Mutation:} changed \texttt{\&\&} to \texttt{||} in the \texttt{if} condition
\end{itemize}


**Effect of the mutation:**  
The mutant’s condition will return early if \emph{either} `x` is provided (regardless of the C call’s result) \emph{or} the C call succeeds (even if `x` is missing).  

**Behavioral difference:**  
- Original: early return only when `x` is present \emph{and} passes the C-level boolean check.  
- Mutant: early return also when `x` is provided but fails the C check, or when `x` is missing but the C check (on `NULL`) passes (if that ever occurs).

**Conclusion:**  
**NOT EQUIVALENT**
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    GPT-4.1 correctly spotted the operator swap in the `if` condition and cited the precise code change.
  
  \item \textbf{Reasoning Quality:}  
    The model’s explanation clearly contrasts the two logical paths and illustrates how the mutant condition diverges from the original in edge cases (missing `x` or failed C-call).
  
  \item \textbf{Validation and Test Coverage:}  
    In practice, this mutant will alter behavior for inputs where `x` is provided but not a boolean, causing an early return instead of proceeding to error handling, and possibly for missing `x` if the C check handles `NULL`. Since the existing test suite does not cover these specific edge cases, the mutant remains “survived,” confirming its validity and highlighting another gap in coverage.
\end{enumerate}

\paragraph{Third mutant under inspection}
\begin{itemize}
  \item \textbf{Mutant ID:} new-data-frame.R\_new-data-frame.R\_002.R  
  \item \textbf{Mutation:} changed logical operator from \texttt{!=} to \texttt{==} in \texttt{validate\_data\_frame}
  \item \textbf{Original code (lines 14–21, trimmed):}
  \begin{lstlisting}[language=R]
  validate_data_frame <- function(x) {
    if (length(unique(lengths(x))) != 1) {
      cli::cli_abort('All elements in a data.frame must be of equal length')
    }
    if (is.null(names(x))) {
      cli::cli_abort('columns must be named')
    }
  }
  \end{lstlisting}
  \item \textbf{Mutated code:}
  \begin{lstlisting}[language=R]
    if (length(unique(lengths(x))) == 1) {
  \end{lstlisting}
\end{itemize}

\paragraph{Answer received from GPT-4.1}
\begin{quote}
DONT KNOW  
\textbf{Reasoning:}  
The change flips the error condition—now aborting when lengths are all equal rather than when they differ. Without knowledge of downstream usage, intended preconditions, or exhaustive test cases, it’s impossible to determine if this mutation preserves observable behavior under all valid inputs.
\end{quote}

\begin{enumerate}
  \item \textbf{Identification:}  
    GPT-4.1 accurately located and described the operator swap (`!=` → `==`) in the length‐check conditional.
  
  \item \textbf{Reasoning Quality:}  
    The model correctly recognizes that deciding equivalence would require complete specification of valid inputs and outputs; expressing “DONT KNOW” here is prudent.
  
  \item \textbf{Validation and Test Coverage:}  
    No existing tests verify both cases where columns are uniform and where they differ in tandem with name checks. Lacking a formal spec or additional test cases, one cannot conclusively judge equivalence. Hence, the LLM’s uncertainty is justified.
\end{enumerate}




\subsection{chatGPT o1 model results}

